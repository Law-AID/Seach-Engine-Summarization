{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Search-Engine",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "BclEli9mjNMY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzbKQvZJjQ1f",
        "outputId": "23c873c9-536a-4c56-ec51-28cb93b25d1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oic74uLncLXB",
        "outputId": "f6073818-4447-4887-f492-203c1894992c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Input: shah jahan\n",
            "\n",
            "\n",
            "Similar sentences: \n",
            "It was built by Mughal Emperor Shah Jahan in memory of his wife Mumtaz Mahal with construction starting in 1632 AD and completed in 1648 AD, with the mosque, the guest house and the main gateway on the south, the outer courtyard and its cloisters were added subsequently and completed in 1653 AD.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "ps = PorterStemmer()\n",
        "f = open('text.txt')\n",
        "a = sent_tokenize(f.read())\n",
        "\n",
        "# removal of stopwords\n",
        "stop_words = list(stopwords.words('english'))\n",
        "\n",
        "# removal of punctuation signs\n",
        "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
        "s = [(word_tokenize(a[i])) for i in range(len(a))]\n",
        "outer_1 = []\n",
        "\n",
        "for i in range(len(s)):\n",
        "\tinner_1 = []\n",
        "\t\n",
        "\tfor j in range(len(s[i])):\n",
        "\t\t\n",
        "\t\tif s[i][j] not in (punc or stop_words):\n",
        "\t\t\ts[i][j] = ps.stem(s[i][j])\n",
        "\t\t\t\n",
        "\t\t\tif s[i][j] not in stop_words:\n",
        "\t\t\t\tinner_1.append(s[i][j].lower())\n",
        "\t\n",
        "\touter_1.append(set(inner_1))\n",
        "rvector = outer_1[0]\n",
        "\n",
        "for i in range(1, len(s)):\n",
        "\trvector = rvector.union(outer_1[i])\n",
        "outer = []\n",
        "\n",
        "for i in range(len(outer_1)):\n",
        "\tinner = []\n",
        "\t\n",
        "\tfor w in rvector:\n",
        "\t\t\n",
        "\t\tif w in outer_1[i]:\n",
        "\t\t\tinner.append(1)\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tinner.append(0)\n",
        "\touter.append(inner)\n",
        "comparison = input(\"Input: \")\n",
        "\n",
        "\n",
        "check = (word_tokenize(comparison))\n",
        "check = [ps.stem(check[i]).lower() for i in range(len(check))]\n",
        "\n",
        "\n",
        "check1 = []\n",
        "for w in rvector:\n",
        "\tif w in check:\n",
        "\t\tcheck1.append(1) # create a vector\n",
        "\telse:\n",
        "\t\tcheck1.append(0)\n",
        "\n",
        "ds = []\n",
        "\n",
        "for j in range(len(outer)):\n",
        "\tsimilarity_index = 0\n",
        "\tc = 0\n",
        "\t\n",
        "\tif check1 == outer[j]:\n",
        "\t\tds.append(0)\n",
        "\telse:\n",
        "\t\tfor i in range(len(rvector)):\n",
        "\n",
        "\t\t\tc += check1[i]*outer[j][i]\n",
        "\n",
        "\t\tsimilarity_index += c\n",
        "\t\tds.append(similarity_index)\n",
        "\n",
        "\n",
        "ds\n",
        "maximum = max(ds)\n",
        "print()\n",
        "print()\n",
        "print(\"Similar sentences: \")\n",
        "for i in range(len(ds)):\n",
        "\n",
        "\tif ds[i] == maximum:\n",
        "\t\tprint(a[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7ZAS1JrUjH3L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}